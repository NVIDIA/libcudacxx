/*

Copyright (c) 2018-2019, NVIDIA Corporation

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

*/

_LIBCPP_BEGIN_NAMESPACE_CUDA
namespace detail {

static inline __device__ void __cuda_membar_block() { asm volatile("membar.cta;":::"memory"); }
static inline __device__ void __cuda_fence_acq_rel_block() { asm volatile("fence.acq_rel.cta;":::"memory"); }
static inline __device__ void __cuda_fence_sc_block() { asm volatile("fence.sc.cta;":::"memory"); }
static inline __device__ void __atomic_thread_fence_cuda(int memorder, __thread_scope_block_tag) {
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __cuda_fence_acq_rel_block(); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __cuda_membar_block(); break;
#endif // __CUDA_ARCH__ >= 700
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.cta.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.cta.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.volatile.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_32_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_32_block(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_volatile_32_block(ptr, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELAXED: __cuda_load_volatile_32_block(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.cta.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.cta.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.volatile.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_64_block(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_64_block(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_volatile_64_block(ptr, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELAXED: __cuda_load_volatile_64_block(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_32_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.cta.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_32_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.cta.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_volatile_32_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.volatile.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __cuda_store_release_32_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_32_block(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __cuda_membar_block();
    case __ATOMIC_RELAXED: __cuda_store_volatile_32_block(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_64_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.cta.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_64_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.cta.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_volatile_64_block(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.volatile.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __cuda_store_release_64_block(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_64_block(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __cuda_membar_block();
    case __ATOMIC_RELAXED: __cuda_store_volatile_64_block(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.cta.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_compare_exchange_release_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_32_block(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_32_block(ptr, old, old_tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_compare_exchange_volatile_32_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_32_block(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_block_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exchange_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_32_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_exchange_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_32_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_and_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_32_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_and_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_max_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_max_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_32_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_max_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_min_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_min_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_32_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_min_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_or_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_32_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_or_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acquire.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.relaxed.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.release.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.cta.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_sub_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_32_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_sub_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acq_rel_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acquire_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_relaxed_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_release_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_volatile_32_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.cta.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_xor_release_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_32_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_32_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_xor_volatile_32_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_32_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.cta.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_compare_exchange_release_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_64_block(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_64_block(ptr, old, old_tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_compare_exchange_volatile_64_block(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_64_block(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_block_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exchange_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_exchange_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_and_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_and_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_max_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_max_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_max_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_min_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_min_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_min_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_or_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_or_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acquire.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.relaxed.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.release.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.cta.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_sub_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_sub_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acq_rel_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acquire_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_relaxed_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_release_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_volatile_64_block(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.cta.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_block_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_xor_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_xor_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_block_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_block_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_64_block(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_block();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_64_block(ptr, tmp, tmp); __cuda_membar_block(); break;
    case __ATOMIC_RELEASE: __cuda_membar_block(); __cuda_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_64_block(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
static inline __device__ void __cuda_membar_device() { asm volatile("membar.gl;":::"memory"); }
static inline __device__ void __cuda_fence_acq_rel_device() { asm volatile("fence.acq_rel.gpu;":::"memory"); }
static inline __device__ void __cuda_fence_sc_device() { asm volatile("fence.sc.gpu;":::"memory"); }
static inline __device__ void __atomic_thread_fence_cuda(int memorder, __thread_scope_device_tag) {
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __cuda_fence_acq_rel_device(); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __cuda_membar_device(); break;
#endif // __CUDA_ARCH__ >= 700
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.gpu.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.gpu.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.volatile.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_32_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_32_device(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_volatile_32_device(ptr, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELAXED: __cuda_load_volatile_32_device(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.gpu.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.gpu.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.volatile.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_64_device(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_64_device(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_volatile_64_device(ptr, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELAXED: __cuda_load_volatile_64_device(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_32_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.gpu.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_32_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.gpu.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_volatile_32_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.volatile.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __cuda_store_release_32_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_32_device(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __cuda_membar_device();
    case __ATOMIC_RELAXED: __cuda_store_volatile_32_device(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_64_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.gpu.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_64_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.gpu.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_volatile_64_device(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.volatile.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __cuda_store_release_64_device(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_64_device(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __cuda_membar_device();
    case __ATOMIC_RELAXED: __cuda_store_volatile_64_device(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.gpu.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_compare_exchange_release_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_32_device(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_32_device(ptr, old, old_tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_compare_exchange_volatile_32_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_32_device(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_device_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exchange_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_32_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_exchange_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_32_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_and_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_32_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_and_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_max_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_max_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_32_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_max_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_min_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_min_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_32_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_min_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_or_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_32_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_or_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acquire.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.relaxed.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.release.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.gpu.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_sub_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_32_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_sub_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acq_rel_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acquire_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_relaxed_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_release_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_volatile_32_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.gpu.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_xor_release_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_32_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_32_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_xor_volatile_32_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_32_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.gpu.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_compare_exchange_release_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_64_device(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_64_device(ptr, old, old_tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_compare_exchange_volatile_64_device(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_64_device(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_device_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exchange_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_exchange_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_and_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_and_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_max_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_max_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_max_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_min_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_min_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_min_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_or_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_or_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acquire.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.relaxed.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.release.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.gpu.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_sub_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_sub_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acq_rel_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acquire_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_relaxed_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_release_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_volatile_64_device(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.gpu.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_device_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_xor_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_xor_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_device_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_device_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_64_device(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_device();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_64_device(ptr, tmp, tmp); __cuda_membar_device(); break;
    case __ATOMIC_RELEASE: __cuda_membar_device(); __cuda_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_64_device(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
static inline __device__ void __cuda_membar_system() { asm volatile("membar.sys;":::"memory"); }
static inline __device__ void __cuda_fence_acq_rel_system() { asm volatile("fence.acq_rel.sys;":::"memory"); }
static inline __device__ void __cuda_fence_sc_system() { asm volatile("fence.sc.sys;":::"memory"); }
static inline __device__ void __atomic_thread_fence_cuda(int memorder, __thread_scope_system_tag) {
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system(); break;
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __cuda_fence_acq_rel_system(); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE:
    case __ATOMIC_ACQ_REL:
    case __ATOMIC_RELEASE: __cuda_membar_system(); break;
#endif // __CUDA_ARCH__ >= 700
    case __ATOMIC_RELAXED: break;
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.sys.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.sys.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.volatile.b32 %0,[%1];" : "=r"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_32_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_32_system(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_volatile_32_system(ptr, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELAXED: __cuda_load_volatile_32_system(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.acquire.sys.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.relaxed.sys.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_load_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst) {asm volatile("ld.volatile.b64 %0,[%1];" : "=l"(_dst) : "l"(_ptr) : "memory"); }
template<class type, typename _VSTD::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_load_cuda(const volatile type *ptr, type *ret, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_acquire_64_system(ptr, tmp); break;
    case __ATOMIC_RELAXED: __cuda_load_relaxed_64_system(ptr, tmp); break;
#else
    case __ATOMIC_SEQ_CST: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_load_volatile_64_system(ptr, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELAXED: __cuda_load_volatile_64_system(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_32_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.sys.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_32_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.sys.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_volatile_32_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.volatile.b32 [%0], %1;" :: "l"(_ptr),"r"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __cuda_store_release_32_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_32_system(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __cuda_membar_system();
    case __ATOMIC_RELAXED: __cuda_store_volatile_32_system(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_relaxed_64_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.relaxed.sys.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_release_64_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.release.sys.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class _CUDA_A, class _CUDA_B> static inline __device__ void __cuda_store_volatile_64_system(_CUDA_A _ptr, _CUDA_B _src) { asm volatile("st.volatile.b64 [%0], %1;" :: "l"(_ptr),"l"(_src) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_store_cuda(volatile type *ptr, type *val, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_RELEASE: __cuda_store_release_64_system(ptr, tmp); break;
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_RELAXED: __cuda_store_relaxed_64_system(ptr, tmp); break;
#else
    case __ATOMIC_RELEASE:
    case __ATOMIC_SEQ_CST: __cuda_membar_system();
    case __ATOMIC_RELAXED: __cuda_store_volatile_64_system(ptr, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.sys.b32 %0,[%1],%2,%3;" : "=r"(_dst) : "l"(_ptr),"r"(_cmp),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 4);
    memcpy(&old, expected, 4);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_compare_exchange_release_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_32_system(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_32_system(ptr, old, old_tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_compare_exchange_volatile_32_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_32_system(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_system_tag) {
    uint32_t tmp = 0;
    memcpy(&tmp, val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exchange_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_32_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_exchange_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 4);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_32_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_and_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_32_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_and_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_max_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_max_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_32_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_max_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_min_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_min_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_32_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_min_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_or_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_32_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_or_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acquire.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.relaxed.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.release.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.sys.u32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_sub_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_32_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_sub_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acq_rel_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acquire_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_relaxed_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_release_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_volatile_32_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.sys.b32 %0,[%1],%2;" : "=r"(_dst) : "l"(_ptr),"r"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==4, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint32_t tmp = 0;
    memcpy(&tmp, &val, 4);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_xor_release_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_32_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_32_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_xor_volatile_32_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_32_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 4);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acq_rel.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.acquire.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.relaxed.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.release.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C, class _CUDA_D> static inline __device__ void __cuda_compare_exchange_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _cmp, _CUDA_D _op) { asm volatile("atom.cas.sys.b64 %0,[%1],%2,%3;" : "=l"(_dst) : "l"(_ptr),"l"(_cmp),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ bool __atomic_compare_exchange_cuda(volatile type *ptr, type *expected, const type *desired, bool, int success_memorder, int failure_memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0, old = 0, old_tmp;
    memcpy(&tmp, desired, 8);
    memcpy(&old, expected, 8);
    old_tmp = old;
    switch (__stronger_order_cuda(success_memorder, failure_memorder)) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_acquire_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_compare_exchange_acq_rel_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_compare_exchange_release_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_relaxed_64_system(ptr, old, old_tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_compare_exchange_volatile_64_system(ptr, old, old_tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_compare_exchange_volatile_64_system(ptr, old, old_tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_compare_exchange_volatile_64_system(ptr, old, old_tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    bool const ret = old == old_tmp;
    memcpy(expected, &old, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_exchange_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.exch.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ void __atomic_exchange_cuda(volatile type *ptr, type *val, type *ret, int memorder, __thread_scope_system_tag) {
    uint64_t tmp = 0;
    memcpy(&tmp, val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_exchange_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_exchange_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_exchange_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_exchange_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_exchange_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(ret, &tmp, 8);
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_add_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.add.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_add_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_and_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.and.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_and_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_and_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_and_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_and_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_and_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_and_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_max_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.max.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_max_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_max_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_max_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_max_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_max_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_max_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_min_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.min.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_min_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_min_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_min_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_min_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_min_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_min_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_or_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.or.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_or_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_or_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_or_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_or_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_or_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_or_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acq_rel.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.acquire.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.relaxed.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.release.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_sub_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { _op = -_op;
asm volatile("atom.add.sys.u64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_sub_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_sub_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_sub_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_sub_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_sub_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_sub_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acq_rel_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acq_rel.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_acquire_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.acquire.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_relaxed_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.relaxed.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_release_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.release.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class _CUDA_A, class _CUDA_B, class _CUDA_C> static inline __device__ void __cuda_fetch_xor_volatile_64_system(_CUDA_A _ptr, _CUDA_B& _dst, _CUDA_C _op) { asm volatile("atom.xor.sys.b64 %0,[%1],%2;" : "=l"(_dst) : "l"(_ptr),"l"(_op) : "memory"); }
template<class type, typename cuda::std::enable_if<sizeof(type)==8, int>::type = 0>
__device__ type __atomic_fetch_xor_cuda(volatile type *ptr, type val, int memorder, __thread_scope_system_tag) {
    type ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_xor_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_xor_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_xor_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_xor_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_xor_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_add_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_system_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}
template<class type>
__device__ type* __atomic_fetch_sub_cuda(type *volatile *ptr, ptrdiff_t val, int memorder, __thread_scope_system_tag) {
    type* ret;
    uint64_t tmp = 0;
    memcpy(&tmp, &val, 8);
    tmp = -tmp;
    tmp *= sizeof(type);
    switch (memorder) {
#if __CUDA_ARCH__ >= 700
    case __ATOMIC_SEQ_CST: __cuda_fence_sc_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_acquire_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_ACQ_REL: __cuda_fetch_add_acq_rel_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELEASE: __cuda_fetch_add_release_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_relaxed_64_system(ptr, tmp, tmp); break;
#else
    case __ATOMIC_SEQ_CST:
    case __ATOMIC_ACQ_REL: __cuda_membar_system();
    case __ATOMIC_CONSUME:
    case __ATOMIC_ACQUIRE: __cuda_fetch_add_volatile_64_system(ptr, tmp, tmp); __cuda_membar_system(); break;
    case __ATOMIC_RELEASE: __cuda_membar_system(); __cuda_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
    case __ATOMIC_RELAXED: __cuda_fetch_add_volatile_64_system(ptr, tmp, tmp); break;
#endif // __CUDA_ARCH__ >= 700
    default: assert(0);
    }
    memcpy(&ret, &tmp, 8);
    return ret;
}

}
_LIBCPP_END_NAMESPACE_CUDA
